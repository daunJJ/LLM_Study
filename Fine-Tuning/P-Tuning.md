## P-Tuning

### 기존 연구의 한계점

- 기반 논문: Liu, Xiao, et al. "GPT understands, too." AI Open (2023).
- **P-Tuning의 핵심 idea** : fine-tuning 과정시 pre-train이 끝난 파라미터 w_0를
고정하고 Template 생성을 위한 Prompt Encoder만을 학습시킴
    - 기존의 Discrete Prompt Search

      <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/68954199-d345-4b53-963f-47e16617222f" width="400"/>

        - 수동으로 여러 개의 단어 조합
        - 정답 Y가 나올 확률이 높은 단어의 조합을 찾아 템플릿화
        - 연속적인 벡터가 아닌 별개의(discrete한) 벡터로 구성됨
    - **P-tuning**

      <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/b93f2ba1-ed1e-43af-a3d0-f60aac0bed6a" width="400"/>

        - pseudo(휴도) Prompts라는 프롬프트임을 나타내는 특수 토큰을 만듦
        - 의미없는 휴도 토큰들로부터 프롬프트 인코더를 활용해서 임베딩된 벡터 형식으로 템플릿을 구성
        - 수동으로 적절한 단어를 하나하나 찾는게 아니라, 휴도 인코더가 그래디언트 디센트로 학습하여 연속적인 템플릿 벡터 토큰을 직접 찾음
        - 프롬프트 인코더만 학습하는 형식으로 fine-tuning
            
            ⇒ Continuous한 프롬프트 템플릿을 직접 만드는 게 핵심
            
- **GPT understans too인 이유**
    
    : GPT의 성능이 약하다고 판단되던 영역인 자연어이해(NLU)영역에서 좋은 성능을 냄
    
- **Pre-trained 언어 모델의 카테고리**
    - 자연어 생성(NLG)을 위한 단방향 언어 모델 (GPT 등)
    - 자연어 이해(NLU)를 위해 양방향 언어 모델 (BERT 등)
    - 하이브리드 언어 모델 (XLNet, UniLM 등)
    
    → GPT는 NLU 작업에서 성능이 떨어짐
    
- **거대한 언어 모델이 가졌던 치명적인 문제점**
    - 하위 작업에서 미세 조정하는 것이 모델들에게 거의 효과가 없음
    - 따라서 수작업으로 제작된 프롬프트를 활용하였음
    - 수작업 프롬프트가 자연어이해를 위해 최고의 성능을 발휘함을 찾았지만
    - 수작업 프롬프트 검색은 비실용적으로 큰 검증 세트에 의존하며, 성능의 변동성이 큼
    - 또한 한 단어의 변화가 극적인 차이를 초래
    - 프롬프트 엔지니어링은 테스트 세트에 과적합하는 것을 의미함
    
- **이산 프롬프트의 자동 검색(Discrete Prompt Search)**
    - 수동으로 템플릿을 찾는 수고를 줄이고자하는 동기부여로 시작
    - 예시) AutoPrompt
        - 후보군의 토큰 중에서 T에 어울리는 토큰을 자동으로 매칭하는 알고리즘
        - 그러나 여전히 단어가 discrete이라는 한계 → 단어 하나에 의해 성능이 크게 변
        
          <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/d8c39ca8-2f24-4ee9-bc23-c6ad6c170912" width="430"/>

- **이를 개선한 P-tuning**
    - 사전 훈련된 언어 모델의 입력 임베딩을, 그것의 미분가능한 출력 임베딩으로 교체

### P-tuning의 개발

- **P-tuning 소개**
    - 연속적 공간에서 자동으로 프롬프트를 검색하는 새로운 방법
    - 연속적인 자유 매개변수(continuous free parameters)를 활용하여 사전 훈련된 언어 모델의 입력으로 공급되는 프롬프트로 사용
    - GPT는 생성할 수 있지만 이해하지 못한다는 고정관념을 깸
    
- **P-tuning이 기여한 바**
    - GPT가 자연어 이해에서 BERT와 경쟁력있게 작동할 수 있음을 보임
    - few shot과 완전한 지도 설정에서 GPT와 BERT 모두를 향상시키는 일반적인 방법임을 보여줌
    
    → 이는 언어 모델이 우리가 이전에 생각했던 것보다 사전 훈련 중에 더 많은 세계 지식과 이전 작업 지식을 습득했음을 나타냄
    
- **두 NLU 벤치마크에서의 성능**
    - LAMA 지식 탐사에서 수작업 프롬프트보다 26-41% 향상되어 최대 64%
    - SuperGlue에서는 few-shot과 지도 시나리오에서 P-tuning → Bert를 능가
    
- **모델 아키텍쳐**
    - 이산 입력 토큰의 연속 → 임베딩 레이어를 거침 → 입력 임베딩으로 매핑
    - 프롬프트 p의 기능: 컨텍스트 x, 대상 y 및 자체를 템플릿 T로 구성하는 것
        - 템플릿: 영국의 수도는 [MASK]입니다
        - 프롬프트: … 의 수도는 … 입니다
        - 컨텍스트: 영국
        - 대상: [MASK]
    - 템플릿 T = {[P_0:i], x, [P_i+1:m], y}
        - 앞, 뒤 임베딩된 프롬프트, 임베딩된 input x, 임베딩된 정답 y
        
          <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/079af379-5285-4b61-9340-9a070c67c114" width="300"/>

    - 이때 input prompt [P_i]를 단어가 아닌 의사 토큰(pseudo tokens)으로 간주
    - 학습이 끝난 프롬프트 인코더가 슈도 토큰을 적합한 임베딩 벡터로 변경해줌
    - 이 임베딩 벡터가 h

      <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/29b2d35d-ae84-42c5-af9c-666edf575737" width="300"/>

    - 하류 손실 함수 L을 사용하여 연속적인 프롬프트 h를 미분 최적화

      <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/e6b9cf4f-ece0-4fbf-bd29-41779af4fe81" width="400"/>

    - Pretrained LM은 freezing 한 후 프롬프트 인코더만 학습 (보통은 LSTM)
    
- **최적화**
    - 연속적인 프롬프트 훈련의 2가지 문제점
        - 단어 임베딩 e는 사전 훈련 후 이미 이산적 → 최적화 도구는 지역 최소값에 빠지기 쉬움
        - 프롬프트 임베딩들이 독립적이 아닌 의존적일 수 있도록 서로 연결하는 매커니즘이 필요
    - 해결 방안
        - 연속성과 연관성 문제를 해결하기 위해 경량 신경망을 포함하는 프롬프트 인코더를 사용하여 h를 시퀀스로도 모델링
        - ReLU 활성화 두 층 다층 퍼셉트론(MLP)과 양방향 LSTM을 선택

          <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/6e173e6b-3f82-414c-839f-85fd2c47d103" width="300"/>

        - LSTM의 헤드는 이는 사전 훈련된 모델보다 크기의 차수가 작으며
        - 추론에서는 출력 임베딩 h만 필요로 하며 LSTM 헤드는 폐기할 수 있음
        - 추가로, 앵커 토큰을 추가하는 것이 도움이 된다는 것을 발견
            - 몇 개의 중요한 단어는 미리 해당 토큰의 임베딩 값으로 지정
            - [PRE][prompt tokens][HYP]?[prompt tokens][MASK] 내의 토큰 "?"은 앵커 토큰
            
- **요약**
    - 프롬프트 엔지니어링을 대체할 수 있는 프롬프트인코더로 LSTM 을 사용
    - 프롬프트라는 슈도 껍데기 토큰이 들어오면 앞, 뒤 템플릿에 task에 맞게 학습된 LSTM의 임베딩 벡터를 붙여 Continuous 프롬프트를 생성
    - 수동으로 프롬프트를 생성하는 것보다 휠씬 효과적
    - GPT 모델의 자연어 이해 영역에서의 성능을 증명
      
- **장점**
    - 수동으로 템플릿을 찾아야 했던 프롬프트 엔지니어링 과정을 생략할 수 있음
