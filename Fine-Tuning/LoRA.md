## LoRA 기법 리뷰

:Low-Rank Adaptation of LLMs

### 배경

- 사전 훈련된 LLM의 모든 파라미터를 **파인튜닝하는 데에는 시간과 비용이 많이 소모됨**
    - 예를 들어 GPT-3의 경우 175B개의 파라미터
- 파인튜닝의 단점을 극복하기 위해 이전에는 **일부 파라미터만 적응시키거나 새로운 작업을 위한 외부 모듈을 학습하는 방법** 도입
    - 어댑터 레이어 추가
    - 입력 레이어 활성화의 어떤 형태를 최적화하는  것
    - prefix tuning
    
    → 종종 모델의 깊이를 확장함으로써 **추론 지연을 도입**하거나, 모델의 사용 가능한 **시퀀스의 길이를 줄임**
    
- 학습된 과도하게 매개변수화된 모델이 **실제로는 낮은 본질적 차원에서 존재함**을 발견
    
    → 모델 적합 동안의 가중치 변화 또한 낮은 **본질적 순위**를 가진다는 가설에서 LoRA 접근법 도출
    
- LoRA는
    1. 사전 훈련된 모델 가중치를 고정
    2. 변환 아키텍처 계층에 훈련 가능한 순위 분해 행렬(rank decomposition matrices)을 최적화

### 장점

- 공유 모델을 고정 시키고, 행렬 A와 B를 교체함으로써 효율적으로 작업을 전환 → 작업 전환 오버헤드 감소
- 훈련 가능한 파라미터 수를 10000배, GPU 요구 사항을 3배 줄임 = Low Rank matrices만 최적화
- 모델 파인튜닝과 동등한 성능을 가지며, 훈련 가능한 파라미터가 적고, 높은 훈련 처리량을 가짐
- 어댑터와 달리 추가적인 추론 지연이 없음
- 순위 결핍(rank-deficiency)에 관한 경험적 조사 제공
- 이전의 다른 PEFT 방법론과 독립적이며, 그 중 많은 것과 결합될 수 있음
- 대다수의 모델 매개변수를 공유함으로써 서비스로 배포될 때 빠른 작업 전환이 가능

### 훈련 과정
<img src="https://github.com/daunJJ/LLM_Study/assets/109944763/fdb71630-ef7c-4beb-8896-7ad2d67f693e" width="400"/>
<img src="https://github.com/daunJJ/LLM_Study/assets/109944763/f5463a4e-0adb-4329-a378-d82a813dfdb2" width="250"/>

- 모델 최적화를 위해 Adam 사용
- Transformer MLP 전방 피드 포워드 차원은 4*d_model 사용
- 특정 파라미터 증가량 ∆Φ = ∆Φ(Θ)가 훨씬 더 작은 크기의 파라미터 집합 Θ에 의해 추가로 인코딩되는 더 파라미터 효율적인 접근법을 채택
    - ∆Φ : 사전 훈련된 모델의 모든 파라미터를 최적화
    - ∆Φ(Θ) : 계산 및 메모리 효율적인 저랭크 표현 최적화
- Low Rank Parameterized update matrices
    - W_0 + ∆W = W_0 + BA
    - 훈련 중에는 W_0 가 고정되어 경사 업데이트를 받지 않으며, A와 B는 훈련 가능한 매개변수를 포함
    - A에는 무작위 가우시안 초기화를 사용 ; A = d x r
    - B에는 0을 사용 ; B = r x d
    - α/r로 ∆W x를 스케일링
    - α를 조정하는 것은 학습률을 조정하는 것과 대략적으로 같음
- 다른 하위 작업으로 전환할 필요가 있을 때, 우리는 BA를 빼서 W_0를 복구하고 다른 B′A′를 더할 수 있음

### 향후 방향성

- 사전 훈련 중에 학습된 특징들이 하위 작업에서 어떻게 잘 수행되도록 변환되는가?의 답변
- LoRA를 적용할 가중치 행렬을 선택하는 방법(기존에는 휴리스틱에 의존)
- ∆W의 순위 결함(rank-deficiency)은 W도 순위 결함일 수 있음
