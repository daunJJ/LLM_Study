## 딥러닝 기본 개념

### 1) 퍼셉트론

- **초기 형태의 인공 신경망**으로 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘
- 실제 뇌를 구성하는 **신경 세포 뉴런**의 동작과 유사함

    <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/72417b8f-8e6f-4aa9-8614-4d4a4cc8e971" width="400"/>

- 신경 세포 뉴런의 입력 신호와 출력 신호가 퍼셉트론에서 각각 입력값과 출력값에 해당함

  <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/e1ec115d-77cd-4004-b8e5-87247dafcaff" width="200"/>

- x는 입력값을 의미하며, w는 가중치(Weight), y는 출력값, 그림 안의 원은 인공 뉴런에 해당함
- 각각의 입력값에는 각각의 가중치가 존재하는데, 이때 가중치의 값이 크면 클수록 해당 입력 값이 중요하다는 것을 의미함
- 이렇게 뉴런에서 출력값을 변경시키는 함수를 **활성화 함수**(Activation Function)라고 함

**(1) 단층 퍼셉트론**

<img src="https://github.com/daunJJ/LLM_Study/assets/109944763/cbc20113-5008-4256-88de-e9ca1c99a980" width="250"/>


- 단층 퍼셉트론은 값을 보내는 단계와 값을 받아서 출력하는 두 단계로만 이루어짐
- 이 두 개의 층을 입력층(input layer)과 출력층(output layer)이라고 함

**(2) 다층 퍼셉트론** 

<img src="https://github.com/daunJJ/LLM_Study/assets/109944763/ca4d6a73-b615-474f-9f62-5cbf5c2fa68e" width="280"/>

- 단층 퍼셉트론과 다른 점은, 단층 퍼셉트론은 입력층과 출력층만 존재하지만 다층 퍼셉트론은 중간에 층을 더 추가하였다는 점
- 이렇게 입력층과 출력층 사이에 존재하는 층을 **은닉층(hidden layer)**이라고 함

    <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/c4b08083-8889-42d4-9fb9-104cd18aba12" width="300"/>

- 위와 같이 은닉층이 2개 이상인 신경망을 **심층 신경망(Deep Neural Network, DNN)**
 이라고 함

### 2) 인공신경망 기본 구조

- 피드 포워드 신경망: 은닉층에서 활성화 함수를 지난 값이 오직 출력층 방향으로 향함

    <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/2fa20239-e54d-4a25-b705-da66d55ba581" width="250"/>

- 순환 신경망: 은닉층의 출력값을 출력층으로도 보내지만, 동시에 다시 은닉층의 입력으로 사용됨

    <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/0fbf4b58-9c5c-4dfa-946b-b7b4a6db97e7" width="250"/>

### 3) 활성화 함수

- 은닉층과 출력층의 뉴런에서 출력값을 결정하는 함수
- 시그모이드함수, 탄젠트함수, 렐루함수, 리키함수, 소프트맥스 함수 등등 많지만 두가지만 소개할 것

    <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/14452560-be62-4c8f-aed7-97ec2d83a57b" width="250"/>

**(1) 인공신경망 활성화 함수의 특징 : 비선형 함수** 

- 선형함수: 출력이 입력의 상수배만큼 변하는 함수, 예를 들어 f(x)=wx+b
라는 함수가 있을 때 이 식을 그래프로 시각화하면 직선임
- 비선형 함수는 직선 1개로는 그릴 수 없는 함수
- 인공 신경망의 능력을 높이기 위해서는 은닉층을 계속해서 추가해야 하는데, 활성화 함수로 선형 함수를 선택하고 층을 계속 쌓는다고 가정하면, 은닉층을 여러번 추가하더라도 1회 추가한 것과 차이가 없음 → 따라서 비선형 함수 이용
    
    f(x) = wx  → f(f(f(x))) = w * w * w * x = kx
    

**(2) 시그모이드 함수 (Sigmoid function)**

<img src="https://github.com/daunJJ/LLM_Study/assets/109944763/1423ec03-d10f-4e75-be24-01b0656ed3bd" width="300"/>

- 인공 신경망 학습과정:
    - 입력에 대해서 순전파(forward propagation) 연산
    - 순전파 연산을 통해 나온 예측값과 실제값의 오차를 손실 함수(loss function)을 통해 계산
    - 그리고 이 손실을 미분을 통해서 기울기(gradient)를 구함
    - 이를 통해 출력층에서 입력층 방향으로 가중치와 편향을 업데이트 하는 과정인 역전파(back propagation)를 수행
- 시그모이드 함수의 문제점은 미분을 해서 기울기(gradient)를 구할 때 발생
- 출력값이 0 또는 1에 가까워지면, 그래프의 기울기가 완만해지는 모습을 볼 수 있음
- 시그모이드 함수를 활성화 함수로하는 인공 신경망의 층을 쌓는다면, 역전파 과정에서 0에 가까운 값이 누적해서 곱해지게 되면서, 앞단에는 기울기가 잘 전달되지 않게 됨
    
    → 기울기 소실 문제 
    

**(3) 하이퍼볼릭탄젠트 함수 (tanh)** 

<img src="https://github.com/daunJJ/LLM_Study/assets/109944763/22a01b2b-a9b7-4982-9538-5c161c757b51" width="300"/>

- 입력값을 -1과 1사이의 값으로 변환
- 미분시, 시그모이드 함수와 같은 문제가 발생하지만 시그모이드 함수보다는 전반적으로 큰 값이 나오게 됨
- 기울기 소실 증상이 적은 편이며 은닉층에서 시그모이드 함수보다는 선호됨

### 4) 손실 함수

<img src="https://github.com/daunJJ/LLM_Study/assets/109944763/2a7216dd-4825-44f2-b328-e8c9522d6c89" width="400"/>

- 손실 함수는 실제값과 예측값의 차이를 수치화해주는 함수
- 오차가 클 수록 손실 함수의 값은 크고 오차가 작을 수록 손실 함수의 값은 작아짐
- 손실 함수의 값을 최소화하는 두 개의 매개변수인 가중치 w와 편향 b의 값을 찾는 것이 딥 러닝의 학습 과정
- 회귀에서는 평균 제곱 오차(MSE), 분류 문제에서는 크로스 엔트로피를 주로 손실 함수로 사용

### 5) 옵티마이저

<img src="https://github.com/daunJJ/LLM_Study/assets/109944763/94c0c6e7-cf16-4440-b52d-ee75a46b7edd" width="400"/>

- 손실 함수의 값을 줄여나가면서 학습하는 방법은 어떤 옵티마이저를 사용하느냐에 따라 달라짐
- 에포크, 내부 노드(뉴런) 수, 드롭아웃(Dropout) 등등 모델을 생성할 때 수많은 파라미터를 조정하게 되는데 그 중에 가장 드라마틱하고 쉽게 바꿔주는 것이 옵티마이저
- 경사 하강법, 모멘텀(momentum), Adam 등 다양한 옵티마이저가 있음

### 6) 순전파와 역전파

<img src="https://github.com/daunJJ/LLM_Study/assets/109944763/92701d86-03b4-4bda-a969-266f497df3ec" width="330"/>

- 위 인공신경망은 입력층, 은닉층, 출력층 이렇게 3개의 층을 가짐
- 또한 두 개의 입력과, 두 개의 은닉층 뉴런, 두 개의 출력층 뉴런을 사용
- 은닉층과 출력층의 모든 뉴런은 활성화 함수로 시그모이드 함수를 사용
- 변수 z는 이전층의 모든 입력이 각각의 가중치와 곱해진 값들이 모두 더해진 가중합을 의미 (이 값은 뉴런에서 아직 시그모이드 함수를 거치지 않은 상태)
- h 또는 o는 z가 시그모이드 함수를 지난 후의 값으로 각 뉴런의 출력값을 의미

**(1) 순전파** 

- 각 입력은 입력층에서 은닉층 방향으로 향하면서 각 입력에 해당하는 가중치와 곱해지고, 결과적으로 가중합으로 계산되어 은닉층 뉴런의 시그모이드 함수의 입력값이 됨
    - z1 = w1x1 + w2x2
    - z2 = w3x1 + w4x2
- z1과 z2는 각각의 은닉층 뉴런에서 시그모이드 함수를 지나게 되는데 시그모이드 함수가 리턴하는 결과값은 은닉층 뉴런의 최종 출력값
    - h1 = sigmoid(z1)
    - h2 = sigmoid(z2)
- 이 과정을 한번 더 반복하여 최종 예측값 o1, o2 얻음
- 예측값과 실제값의 오차를 계산하기 위한 오차 함수를 선택, 손실함수를 이용하여 전체 오차 Etotal을 구함

**(2) 역전파** 

- 순전파가 입력층에서 출력층으로 향한다면 역전파는 반대로 출력층에서 입력층 방향으로 계산하면서 가중치를 업데이트

    <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/e2118771-2e67-47f8-af3a-7447c7f976e2" width="250"/>

- 4개의 새로운 w(가중치)를 구해야 하는데, 이는 미분의 연쇄 법칙에 따라 구함

    <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/b9c9680b-545f-4246-ad71-08b15a473cb3" width="300"/>
    
- 이 과정을 반복하여 가중치를 업데이트
- 업데이트 된 가중치에 대해서 다시 한 번 순전파를 진행하여 오차가 감소하였는지 확인함
- 인공 신경망의 학습은 오차를 최소화하는 가중치를 찾는 목적으로 순전파와 역전파를 반복하는 것을 말함
