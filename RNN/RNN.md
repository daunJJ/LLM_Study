## RNN 기본 구조

### 1) RNN이란?

- **Recurrent Neural Network로 입력과 출력을 시퀀스 단위로 처리하는 시퀀스(Sequence) 모델**
- 여기서 시퀀스는 단어들로 이루어진 하나의 문장을 뜻함
- 시퀀스를 처리하기 위해 고안된 모델 중 하나로, 가장 기본적인 인공신경망 시퀀스 모델
- LSTM이나 GRU 또한 근본적으로 RNN에 속함

### 2) 순환신경망(**Recurrent Neural Network, RNN)**

- 피드포워드 신경망과 달리, 순환신경망은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보냄
- RNN의 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드를 셀(cell)이라고 함
- 셀은 이전의 값을 기억하려고 하는 일종의 메모리 역할을 수행하므로 이를 **메모리 셀**
 또는 **RNN 셀**이라고 표현
- 이 셀의 출력을 은닉상태(hidden state)라고 함

    <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/4a9c2b43-b1ff-4d73-bd1b-1d280648a63e" width="350"/>

- 위 그림은 RNN의 사이클로 재귀 형태로 표현하거나, 여러 시점을 펼쳐서 표현하기도 함

    <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/1be4e2ee-84a1-43d1-8b3b-751883816a3c" width="400"/>

- RNN은 입력과 출력의 길이를 다르게 설계 할 수 있으므로 다양한 용도로 사용 가능
- 일 대 다 구조 - 하나의 이미지 입력으로 사진의 제목을 출력하는 이미지 캡셔닝 작업
- 다 대 일 구조 - 입력 문서의 긍/부정을 판별하는 감성 분류
- 다 대 다 구조 - 사용자의 문장 입력으로 대답 문장을 출력하는 챗봇 또는 번역기 또는 태깅 작업

    <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/ffdafef6-a95d-4c00-aafe-b25a40c55192" width="350"/>

- 현재 시점 t에서 입력값은 x(t), 출력값은 y(t), 은닉상태값은 h(t)
- 은닉층의 메모리셀은 h(t)를 계산하기 위해 입력층의 가중치(Wxh)와 시점 t-1의 은닉상태값인 h(t-1)을 위한 가중치(Whh), 총 두 개의 가중치를 가짐
- 자연어 처리에서 RNN의 입력 x(t)는 대부분의 경우 단어 벡터로 간주할 수 있는데, 단어 벡터의 차원을 d라고 하고, 은닉 상태의 크기를 Dh라고 하였을 때 각 벡터와 행렬의 크기는 다음과 같음

    <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/303382fc-5fc3-40ed-b3a3-155bb9f314b0" width="450"/>

- h(t)를 계산하기 위한 활성화 함수로는 주로 하이퍼볼릭탄젠트 함수(tanh)가 사용됨
- 출력값 y(t)를 계산하기 위한 활성화 함수는 푸는 문제에 따라 다름
    
    → 이진 분류를 해야하는 경우: 시그모이드 함수 사용
    
    → 다중 클래스 분류를 해야하는 경우: 소프트맥스 함수 사용
    

### 3) RNN 기본 구조 예제

<img src="https://github.com/daunJJ/LLM_Study/assets/109944763/ae2b79a8-6dcb-41f3-b271-c14fc63e18fd" width="350"/>

- ‘hell’을 넣으면 ‘o’를 반환하게 해 결과적으로는 ‘hello’를 출력하게 만들고 싶음
- 학습데이터의 글자는 ‘h’, ‘e’, ‘l’, ‘o’ 를 one-hot-vector로 바꾸면 각각 [1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]이 됨
- 첫번째값 x1이 랜덤값 h0를 만나 은닉상태값인 h1을 만들고, 이를 바탕으로 출력값인 y1을 생성, 두번째값 x2는 이전 은닉상태값인 h1과 만나 h2를 만들고 y2 생성
- 이렇게 세번째, 네번째 단계도 갱신하는 과정이 순전파
- h의 정답은 e, e 다음은 l, l 다음은 l.. 이런 식으로 모델에 정답이 존재, 역전파를 수행해 모델이 파라미터를 적절히 갱신해 나가며 최적 모델을 생성함

### 4) 다양한 순환 신경망

<img src="https://github.com/daunJJ/LLM_Study/assets/109944763/739c7a06-857f-405e-a07f-d08dcbc3ca47" width="200"/>

- 다수의 은닉층을 가진 깊은 순환신경망

<img src="https://github.com/daunJJ/LLM_Study/assets/109944763/16ad57ca-52bb-4bd3-ad87-7d4f109cb62d" width="300"/>

- 이전 시점의 입력 뿐만 아니라, 이후 시점의 입력도 예측에 기여하는 양방향 순환 신경망

### 5) LSTM

- RNN은 훈련 스텝을 거치며 일부 정보가 사라지는 단기 기억의 문제점을 가짐
- 이러한 단점을 해소하는 장기 메모리를 가진 셀 중 하나가 LSTM
- 삭제 게이트를 통해 장기 상태에 저장할 것, 버릴 것, 읽어들일 것을 학습

    <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/386cff33-7828-4925-a20f-52867ec058a5" width="350"/>
