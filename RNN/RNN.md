# 분석 멘멘 RNN study

## 딥러닝 기본 개념

### 1) 퍼셉트론

- **초기 형태의 인공 신경망**으로 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘
- 실제 뇌를 구성하는 **신경 세포 뉴런**의 동작과 유사함

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled.png)

- 신경 세포 뉴런의 입력 신호와 출력 신호가 퍼셉트론에서 각각 입력값과 출력값에 해당함

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%201.png)

- x는 입력값을 의미하며, w는 가중치(Weight), y는 출력값, 그림 안의 원은 인공 뉴런에 해당함
- 각각의 입력값에는 각각의 가중치가 존재하는데, 이때 가중치의 값이 크면 클수록 해당 입력 값이 중요하다는 것을 의미함
- 이렇게 뉴런에서 출력값을 변경시키는 함수를 **활성화 함수**(Activation Function)라고 함

**(1) 단층 퍼셉트론**

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%202.png)

- 단층 퍼셉트론은 값을 보내는 단계와 값을 받아서 출력하는 두 단계로만 이루어짐
- 이 두 개의 층을 입력층(input layer)과 출력층(output layer)이라고 함

**(2) 다층 퍼셉트론** 

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%203.png)

- 단층 퍼셉트론과 다른 점은, 단층 퍼셉트론은 입력층과 출력층만 존재하지만 다층 퍼셉트론은 중간에 층을 더 추가하였다는 점
- 이렇게 입력층과 출력층 사이에 존재하는 층을 **은닉층(hidden layer)**이라고 함

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%204.png)

- 위와 같이 은닉층이 2개 이상인 신경망을 **심층 신경망(Deep Neural Network, DNN)**
 이라고 함

### 2) 인공신경망 기본 구조

- 피드 포워드 신경망: 은닉층에서 활성화 함수를 지난 값이 오직 출력층 방향으로 향함

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%205.png)

- 순환 신경망: 은닉층의 출력값을 출력층으로도 보내지만, 동시에 다시 은닉층의 입력으로 사용됨

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%206.png)

### 3) 활성화 함수

- 은닉층과 출력층의 뉴런에서 출력값을 결정하는 함수
- 시그모이드함수, 탄젠트함수, 렐루함수, 리키함수, 소프트맥스 함수 등등 많지만 두가지만 소개할 것

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%207.png)

**(1) 인공신경망 활성화 함수의 특징 : 비선형 함수** 

- 선형함수: 출력이 입력의 상수배만큼 변하는 함수, 예를 들어 f(x)=wx+b
라는 함수가 있을 때 이 식을 그래프로 시각화하면 직선임
- 비선형 함수는 직선 1개로는 그릴 수 없는 함수
- 인공 신경망의 능력을 높이기 위해서는 은닉층을 계속해서 추가해야 하는데, 활성화 함수로 선형 함수를 선택하고 층을 계속 쌓는다고 가정하면, 은닉층을 여러번 추가하더라도 1회 추가한 것과 차이가 없음 → 따라서 비선형 함수 이용
    
    f(x) = wx  → f(f(f(x))) = w * w * w * x = kx
    

**(2) 시그모이드 함수 (Sigmoid function)**

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%208.png)

- 인공 신경망 학습과정:
    - 입력에 대해서 순전파(forward propagation) 연산
    - 순전파 연산을 통해 나온 예측값과 실제값의 오차를 손실 함수(loss function)을 통해 계산
    - 그리고 이 손실을 미분을 통해서 기울기(gradient)를 구함
    - 이를 통해 출력층에서 입력층 방향으로 가중치와 편향을 업데이트 하는 과정인 역전파(back propagation)를 수행
- 시그모이드 함수의 문제점은 미분을 해서 기울기(gradient)를 구할 때 발생
- 출력값이 0 또는 1에 가까워지면, 그래프의 기울기가 완만해지는 모습을 볼 수 있음
- 시그모이드 함수를 활성화 함수로하는 인공 신경망의 층을 쌓는다면, 역전파 과정에서 0에 가까운 값이 누적해서 곱해지게 되면서, 앞단에는 기울기가 잘 전달되지 않게 됨
    
    → 기울기 소실 문제 
    

**(3) 하이퍼볼릭탄젠트 함수 (tanh)** 

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%209.png)

- 입력값을 -1과 1사이의 값으로 변환
- 미분시, 시그모이드 함수와 같은 문제가 발생하지만 시그모이드 함수보다는 전반적으로 큰 값이 나오게 됨
- 기울기 소실 증상이 적은 편이며 은닉층에서 시그모이드 함수보다는 선호됨

### 4) 손실 함수

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%2010.png)

- 손실 함수는 실제값과 예측값의 차이를 수치화해주는 함수
- 오차가 클 수록 손실 함수의 값은 크고 오차가 작을 수록 손실 함수의 값은 작아짐
- 손실 함수의 값을 최소화하는 두 개의 매개변수인 가중치 w와 편향 b의 값을 찾는 것이 딥 러닝의 학습 과정
- 회귀에서는 평균 제곱 오차(MSE), 분류 문제에서는 크로스 엔트로피를 주로 손실 함수로 사용

### 5) 옵티마이저

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%2011.png)

- 손실 함수의 값을 줄여나가면서 학습하는 방법은 어떤 옵티마이저를 사용하느냐에 따라 달라짐
- 에포크, 내부 노드(뉴런) 수, 드롭아웃(Dropout) 등등 모델을 생성할 때 수많은 파라미터를 조정하게 되는데 그 중에 가장 드라마틱하고 쉽게 바꿔주는 것이 옵티마이저
- 경사 하강법, 모멘텀(momentum), Adam 등 다양한 옵티마이저가 있음

### 6) 순전파와 역전파

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%2012.png)

- 위 인공신경망은 입력층, 은닉층, 출력층 이렇게 3개의 층을 가짐
- 또한 두 개의 입력과, 두 개의 은닉층 뉴런, 두 개의 출력층 뉴런을 사용
- 은닉층과 출력층의 모든 뉴런은 활성화 함수로 시그모이드 함수를 사용
- 변수 z는 이전층의 모든 입력이 각각의 가중치와 곱해진 값들이 모두 더해진 가중합을 의미 (이 값은 뉴런에서 아직 시그모이드 함수를 거치지 않은 상태)
- h 또는 o는 z가 시그모이드 함수를 지난 후의 값으로 각 뉴런의 출력값을 의미

**(1) 순전파** 

- 각 입력은 입력층에서 은닉층 방향으로 향하면서 각 입력에 해당하는 가중치와 곱해지고, 결과적으로 가중합으로 계산되어 은닉층 뉴런의 시그모이드 함수의 입력값이 됨
    - z1 = w1x1 + w2x2
    - z2 = w3x1 + w4x2
- z1과 z2는 각각의 은닉층 뉴런에서 시그모이드 함수를 지나게 되는데 시그모이드 함수가 리턴하는 결과값은 은닉층 뉴런의 최종 출력값
    - h1 = sigmoid(z1)
    - h2 = sigmoid(z2)
- 이 과정을 한번 더 반복하여 최종 예측값 o1, o2 얻음
- 예측값과 실제값의 오차를 계산하기 위한 오차 함수를 선택, 손실함수를 이용하여 전체 오차 Etotal을 구함

**(2) 역전파** 

- 순전파가 입력층에서 출력층으로 향한다면 역전파는 반대로 출력층에서 입력층 방향으로 계산하면서 가중치를 업데이트

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%2013.png)

- 4개의 새로운 w(가중치)를 구해야 하는데, 이는 미분의 연쇄 법칙에 따라 구함

![스크린샷(876).png](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7(876).png)

- 이 과정을 반복하여 가중치를 업데이트
- 업데이트 된 가중치에 대해서 다시 한 번 순전파를 진행하여 오차가 감소하였는지 확인함
- 인공 신경망의 학습은 오차를 최소화하는 가중치를 찾는 목적으로 순전파와 역전파를 반복하는 것을 말함

## RNN 기본 구조

### 1) RNN이란?

- **Recurrent Neural Network로 입력과 출력을 시퀀스 단위로 처리하는 시퀀스(Sequence) 모델**
- 여기서 시퀀스는 단어들로 이루어진 하나의 문장을 뜻함
- 시퀀스를 처리하기 위해 고안된 모델 중 하나로, 가장 기본적인 인공신경망 시퀀스 모델
- LSTM이나 GRU 또한 근본적으로 RNN에 속함

### 2) 순환신경망(**Recurrent Neural Network, RNN)**

- 피드포워드 신경망과 달리, 순환신경망은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보냄
- RNN의 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드를 셀(cell)이라고 함
- 셀은 이전의 값을 기억하려고 하는 일종의 메모리 역할을 수행하므로 이를 **메모리 셀**
 또는 **RNN 셀**이라고 표현
- 이 셀의 출력을 은닉상태(hidden state)라고 함

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%2014.png)

- 위 그림은 RNN의 사이클로 재귀 형태로 표현하거나, 여러 시점을 펼쳐서 표현하기도 함

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%2015.png)

- RNN은 입력과 출력의 길이를 다르게 설계 할 수 있으므로 다양한 용도로 사용 가능
- 일 대 다 구조 - 하나의 이미지 입력으로 사진의 제목을 출력하는 이미지 캡셔닝 작업
- 다 대 일 구조 - 입력 문서의 긍/부정을 판별하는 감성 분류
- 다 대 다 구조 - 사용자의 문장 입력으로 대답 문장을 출력하는 챗봇 또는 번역기 또는 태깅 작업

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%2016.png)

- 현재 시점 t에서 입력값은 x(t), 출력값은 y(t), 은닉상태값은 h(t)
- 은닉층의 메모리셀은 h(t)를 계산하기 위해 입력층의 가중치(Wxh)와 시점 t-1의 은닉상태값인 h(t-1)을 위한 가중치(Whh), 총 두 개의 가중치를 가짐
- 자연어 처리에서 RNN의 입력 x(t)는 대부분의 경우 단어 벡터로 간주할 수 있는데, 단어 벡터의 차원을 d라고 하고, 은닉 상태의 크기를 Dh라고 하였을 때 각 벡터와 행렬의 크기는 다음과 같음

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%2017.png)

- h(t)를 계산하기 위한 활성화 함수로는 주로 하이퍼볼릭탄젠트 함수(tanh)가 사용됨
- 출력값 y(t)를 계산하기 위한 활성화 함수는 푸는 문제에 따라 다름
    
    → 이진 분류를 해야하는 경우: 시그모이드 함수 사용
    
    → 다중 클래스 분류를 해야하는 경우: 소프트맥스 함수 사용
    

### 3) RNN 기본 구조 예제

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%2018.png)

- ‘hell’을 넣으면 ‘o’를 반환하게 해 결과적으로는 ‘hello’를 출력하게 만들고 싶음
- 학습데이터의 글자는 ‘h’, ‘e’, ‘l’, ‘o’ 를 one-hot-vector로 바꾸면 각각 [1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]이 됨
- 첫번째값 x1이 랜덤값 h0를 만나 은닉상태값인 h1을 만들고, 이를 바탕으로 출력값인 y1을 생성, 두번째값 x2는 이전 은닉상태값인 h1과 만나 h2를 만들고 y2 생성
- 이렇게 세번째, 네번째 단계도 갱신하는 과정이 순전파
- h의 정답은 e, e 다음은 l, l 다음은 l.. 이런 식으로 모델에 정답이 존재, 역전파를 수행해 모델이 파라미터를 적절히 갱신해 나가며 최적 모델을 생성함

### 4) 다양한 순환 신경망

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%2019.png)

- 다수의 은닉층을 가진 깊은 순환신경망

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%2020.png)

- 이전 시점의 입력 뿐만 아니라, 이후 시점의 입력도 예측에 기여하는 양방향 순환 신경망

### 5) LSTM

- RNN은 훈련 스텝을 거치며 일부 정보가 사라지는 단기 기억의 문제점을 가짐
- 이러한 단점을 해소하는 장기 메모리를 가진 셀 중 하나가 LSTM
- 삭제 게이트를 통해 장기 상태에 저장할 것, 버릴 것, 읽어들일 것을 학습

![Untitled](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/Untitled%2021.png)

![스크린샷(877).png](%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A6%E1%86%AB%E1%84%86%E1%85%A6%E1%86%AB%20RNN%20study%20b5dd0c82369c45c6a98c90cc9d623175/%25EC%258A%25A4%25ED%2581%25AC%25EB%25A6%25B0%25EC%2583%25B7(877).png)