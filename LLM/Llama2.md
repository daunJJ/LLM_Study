## Llama-2-chat 학습 과정

**1. Llama2 language model pre- training**
- 트랜스포머 모델 기반 다음 토큰을 예측
-  Llama1과 학습과정은 동일하나 토큰 갯수 up & 문맥 길이 up

**2. prompt-response쌍 데이터셋을 모아서 supervised fine-tuning(STF)**
- 프롬프트와 답변 세그먼트 구분을 위해 특별한 토큰 사용
- 답변 토큰에만 역전파 사용

**3. response 정제를 위해 RLHF**
- 유용성(helpfulness)와 안전성(safely) 측면의 두 가지 모델로 구성
- 인간 주석 달기로 데이터 수집
    1. 두 개의 샘플링된 모델 응답 중 하나 선택 
    2. 선택한 응답이 얼마나 더 선호되는지 라벨링
       → Meta 보상 모델링 데이터 
        
- 보상 모델링 (RM)
    - 모델 프롬프트와 응답을 받아 품질(유용성, 안전성)을 나타내는 스칼라 점수 출력
    - 2차로 STF된 모델 체크포인트에서 초기화
    - 1차 pretrained Llama 모델과 다른점: output이 스칼라 보상
    - 보상 모델 훈련을 위한 목적 함수
        - 사람이 레이블링한 점수와 모델이 레이블링한 점수의 차이가 적으면 loss down → 사람과 유사한 판단을 내리도록
    - Meta 보상 모델링 데이터와 오픈 소스 데이터셋을 적절히 섞어 데이터 구성
    - 오래 훈련하면 과적합이 발생할 수 있어 1번의 epoch 동안 훈련
    - 인간의 보상 점수와 보상 모델링의 보상 점수 비교
        - Safety RM과 Helpfulness RM의 성능이 나쁘지 않음
        - 그러나 비교쌍(1개의 프롬프트에 대한 2개의 응답)이 유사해질수록 정확도가 하락
    - 더 많은 주석으로 더 많은 개선이 가능
- RLHF 미세 조정
    - PPO, Rejection Sampling fine-tunig의 2가지 알고리즘 사용 
      → 사용 알고리즘에 따라 Version 다름 
            
    - 최적의 temperature 찾음

**4. Multi-Turn Consistency를 위한 Ghost Attention**
- 여러 턴의 걸친 대화 제어를 위해 동일성 유지가 필요
- 대화 내내 준수되어야 하는 지시사항을 ‘inst’로 정의하여 fine-tuning시 추가 레이블로 학습

## Llama-2-chat 평가

- 세 명의 주석자에게 답변의 품질을 판단하도록 함
- SFT보다 RLHF 를 거쳤을 때 안전성과 도움성 측면에서 성능이 좋음
- 그러나 인간 평가에는 본질적으로 주관적이고 노이즈가 있다는 한계가 있음
- 또한 프롬프트 세트에 코딩이나 추론 관련 프롬프트가 포함되어 있지 않다는 한계 (프롬프트 다양성 한계)

## Llama2 안전성 평가

**Pretraining**

- Meta(페이스북) 사용자 데이터 사용X
- 개인 정보가 많이 포함된 데이터 사용 X
- 범용성을 위해 추가적인 필터링 X
- 인구통계학적 대표성 확인
- 종교, 성별과 성, 국적, 인종, 성적 지향 빈도 확인
- HateBERT 분류기로 데이터 독성 측정 결과 0.2%의 독성
- 언어 식별 → 영어 외 단어를 사용하기에 적합하지 않음
- 진실성, 독성, 편향성 벤치마크를 사용하여 평가
    
    → 독성 메트릭에 성능이 낮은데, 데이터를 필터링하지 않은 결과로 추측
        
**Fine-tuning**

1. 지도학습을 통한 안전 세부 조정
    - 적대적 프롬프트 생성을 위한 지침 설계
        - 위험 카테고리, 공격벡터 → 모범 사례 정의
    - 안전한 모델 응답의 프롬프트와 데모를 수집하여 데이터로 사용
2. 안전 RLHF
    - 인간 어노테이터들은 일련의 지침에 따라 가장 안전한 응답 선택
        
        → 인간 선호도 데이터로 안전 보상 모델을 훈련시킴
        
        - 안전 RLHF 적용 후 위험한 답변에 대해 안전성이 높아짐
        - 안전 RLHF의 적용은 유용성과는 관련이 없음
    - 안전 데이터의 비율을 늘릴때 성능이 크게 향상
        - 유용성 점수는 일정하게 유지
    - 잘못된 거절(안전 우려 때문에 잘못 거절하는 경우)의 측정
        
        → 안전 데이터를 100% 사용해도 0.05%에 불과
            
    3. 안전 문맥 정제 
        - 안전한 프리프롬프트를 모델 앞에 붙임으로써 안전 성능 향상
            - 원래 답변보다 더 나은 보상 모델 점수를 얻는 예시에만 증류된 출력을 유지
        
**Red Teaming**
- 350명의 사람들로 구성되어 모델을 평가
  
**다른 모델과 비교하여 안전성 인간 평가**
- 다른 모델보다 Llama-2-chat의 위험성이 낮음
- 멀티 턴 대화가 단일 턴보다 불안전한 응답을 유발하는 경향
- pretrained Llama2보다 Llama-2-chat에서 진실성, 독성이 크게 개선됨

## Discussion

**관찰된 결과**

- RLHF을 적용했을 때 답변의 변동성이 제거됨
- 온도를 높임으로써 창의성이 요구되는 질문에는 다양한 응답을 제공하지만, 여전히 사실적인 답변에 대한 질문에는 일관된 응답을 제공함
- 시간 개념을 주입해서 날짜에 따라 다른 답변을 제공함
- 도구를 제시해주면 해당 도구를 활용할 수 있음

**한계**

- 사전 훈련 이후 지식 업데이트의 중단
- 사실이 아닌 정보 생성의 가능성
- 환각을 일으킬 가능성
- 영어 이외의 언어에 취약
- 모욕적이고 편향적인 콘텐츠를 생성할 가능성
- 안전 튜닝으로 인해 지나치게 조심스러운 접근 방식을 취할 수 있음
