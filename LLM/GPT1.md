## [Overview] GPT-1,2,3 ì •ë¦¬
|  | GPT-1 | GPT-2 | GPT-3 |
| --- | --- | --- | --- |
| í•™ìŠµ ë°ì´í„°ì…‹ | bookCopus | web-text(Reddit) | common crawl |
| íŒŒë¼ë¯¸í„°  | 1.17ì–µ | 15ì–µ | 1750ì–µ |
| íŠ¹ì§•  | ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¡œ Unsupervised Pre-train í›„ ê°œë³„ Taskì— Supervise fine-tuningì„ í•˜ì˜€ë”ë‹ˆ ì‘ì€ ì§€ë„í•™ìŠµ ë°ì´í„°ì…‹ìœ¼ë¡œë„ ì„±ëŠ¥ì´ ì¢‹ìŒì„ ì¦ëª…  | ë³„ë„ì˜ fine-tuning ê³¼ì • ì—†ì´ë„ Unsupervised Pre-training ë§Œìœ¼ë¡œë„ ë‹¤ì–‘í•œ taskì—ì„œ ì„±ëŠ¥ì´ ì¢‹ìŒì„ ì¦ëª… (zero-shot learning)  | ë³„ë„ì˜ fine-tuning ê³¼ì • ì—†ì´ Unsupervised Pre-training í›„ In-context learningì„ ìˆ˜í–‰í•˜ì—¬ ì˜ˆì‹œì˜ ê°œìˆ˜ê°€  ë§ì•„ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ ì¢‹ì•„ì§ì„ ì¦ëª… |

## GPT-1(2018)

**Overview**

- generative pre-trainingì„ í†µí•´ì„œ ëŒ€ëŸ‰ì˜ corpus ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¨ ë’¤ ê°œë³„ taskì— fine-tuning
- ë‹¨ì¼ ëª¨ë¸ë¡œ ì—¬ëŸ¬ ê°œì˜ sub taskë¥¼ ìˆ˜í–‰ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬
- ì´ë¯¸ì§€ì²˜ë¦¬ì—ì„œ ìœ í–‰í•˜ë˜ <ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¡œ pre-trainingí•œ ë‹¨ì¼ ëª¨ë¸ â†’ ê°œë³„ taskì— ë§ê²Œ fine-tuning>ì„ NLPì— ë„ì…í•œ ìµœì´ˆì˜ ë…¼ë¬¸

**Architecture**

- Transformer ë””ì½”ë”ë¥¼ 12depth ìŒ“ìŒ
- ë¶„ë¥˜, í•¨ì˜ ì¸ì‹, ìœ ì‚¬ì„±, ë‹¤ì¤‘ ì„ íƒ ë“±ì˜ ì„œë¡œ ë‹¤ë¥¸ Taskë“¤ì€ ì›í•˜ëŠ” Inputí˜•íƒœê°€ ë‹¤ë¦„ â†’ Task Specific Transformationì„ ê±°ì³ ë™ì¼í•œ ëª¨ë¸ì— ë„£ì„ ìˆ˜ ìˆë„ë¡ Input ë³€í˜• ì‘ì—…ì„ ê±°ì¹¨

**Parameter**

- GPT-1(1.17ì–µ) â†’ GPT-2(15ì–µ) â†’ GPT-3(1750ì–µ)

## GPT-1(2018) ë…¼ë¬¸ ì •ë¦¬

### **Intro**

- GPTëŠ” Generative Pre-trained Transformerì˜ ì•½ì
- GPT-1ì€ OpenAIì—ì„œ 2018ë…„ì— ê°œë°œí•œ ìµœì´ˆì˜ GPTëª¨ë¸
- ê¸°ë°˜ ë…¼ë¬¸: Improving language understanding by generative pre-training.

### **í•µì‹¬ Contribution**

- í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ë‹¤ì–‘í•œ ìì—°ì–´ì²˜ë¦¬ Taskì— ëŒ€í•´ì„œ state-of-the-art ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ëª¨ë¸
    - ëŒ€ê·œëª¨ unlabeled ë°ì´í„°ì…‹ì„ ì´ìš©í•œ Pre-Training
    - ê°œë³„ Task ì— ë§ê²Œ Fine-Tuning

### **Abstract**

- GPT-1ì€ íŠ¹ì • taskë¥¼ ëª©ì ìœ¼ë¡œ í•˜ì§€ ì•Šê³ , ì¼ë°˜ì ì¸ NLP ëª¨ë¸ë¡œ
task-agnostic(ì´ˆê²½í—˜ì ì¸ ê²ƒì˜ ë³¸ì²´ëŠ” ì¸ì‹ ë¶ˆê°€ëŠ¥)í•˜ê²Œ í•™ìŠµ
- unlabeled í…ìŠ¤íŠ¸ ë¬¶ìŒ ë°ì´í„°ëŠ” í’ë¶€í•œ ë°˜ë©´, ê°ê°ì˜ ê°œë³„ taskì— ëŒ€í•´ì„œ labeledëœ ë°ì´í„°ëŠ” í¬ì†Œ
- GPT-1ì€ generative pre-trainingë¡œ ë¨¼ì € í•™ìŠµí•˜ê³  ê°œë³„ taskì— ëŒ€í•´ì„œ fine-tuningí•  ê²½ìš° ê°ê°ì˜ ê°œë³„ taskì— ëŒ€í•´ í° ì„±ëŠ¥ ê°­ì„ ë³´ì—¬ì£¼ë©´ì„œ state-of-the-artë¥¼ ê°œì„ í•  ìˆ˜ìˆìŒì„ ì¦ëª…
- ê°ê°ì˜ taskì— ìµœì í™”ëœ í˜•íƒœì˜ ëª¨ë¸ê³¼ ë§ì€ íŠœë‹ì´ ë“¤ì–´ê°„ ëª¨ë¸ë“¤ì„ ì´ê¸°ê³  12ê°œì˜ subtaskì¤‘ì—ì„œ 9ê°œì—ì„œ state-of-the-art ì„±ëŠ¥

### **ìì—°ì–´ ì²˜ë¦¬ Task**

- Text  Entailment  (í…ìŠ¤íŠ¸ í•¨ì˜ì¸ì‹)
    - í…ìŠ¤íŠ¸ê°„ì˜ ì˜ë¯¸ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ë¬¸ì œì˜ì—­
    
    - ë¹„êµí•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ Premise pì™€ hypothesis h
    - ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ê´€ê³„ì— ëŒ€í•œ ì •ë‹µ ë ˆì´ë¸”ì€ entailment (í•¨ì˜), contradiction(ëª¨ìˆœ) or neutral(ì¤‘ë¦½)
- Semantic Similarity
    - ë‘ ë¬¸ì¥ì´ ë™ì¼í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ë¥¼ íŒë‹¨í•˜ëŠ” NLP ë¬¸ì œì˜ì—­
    - ì •ë‹µ ë ˆì´ë¸”ì€ same, not same ì´ê±°ë‚˜ ë‘ë¬¸ì¥ê°„ì˜ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ì˜ˆì¸¡
- Reading Comprehension
    - í…ìŠ¤íŠ¸ë¥¼ ì½ê³  í…ìŠ¤íŠ¸ ì•ˆì˜ ë‚´ìš©ì„ ì´í•´í•˜ëŠ”ì§€ë¥¼ í…ŒìŠ¤íŠ¸í•˜ëŠ” NLP ë¬¸ì œì˜ì—­
- Commonsense Reasoning
    - (ì¸ê°„ì´ ë§¤ì¼ ë§ˆì£¼í•˜ëŠ” ì¼ìƒì ì¸ ìƒí™©ì˜ ìœ í˜•ê³¼ ë³¸ì§ˆì„ ì¶”ì¸¡í•˜ëŠ” ì¸ê°„ê³¼ ê°™ì€) ìƒì‹ ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ë¬¸ì œì˜ì—­
    - ë¬¼ë¦¬ì  ëŒ€ìƒì˜ íŠ¹ì„±, ë¶„ë¥˜í•™ì  ì†ì„± ë° ì‚¬ëŒë“¤ì˜ ì˜ë„ì— ëŒ€í•œ íŒë‹¨ì´ í¬í•¨
- Sentiment Analysis
    - í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆëŠ” ê°ì • ìƒíƒœë¥¼ ë§ì¶”ëŠ” ë¬¸ì œì˜ì—­
- Linguistic Acceptability
    - í…ìŠ¤íŠ¸ê°€ ë¬¸ë²•ì ìœ¼ë¡œ(grammatical) ì˜¬ë°”ë¥¸ì§€ë¥¼ í…ŒìŠ¤íŠ¸í•˜ëŠ” NLP ë¬¸ì œì˜ì—­

### **Introduction**

- unlabeled í…ìŠ¤íŠ¸ì—ì„œ í•™ìŠµí•˜ê¸°
    - subtaskì— ëŒ€í•œ supervised learning ê³¼ì •ì„ ì™„í™”í•˜ë ¤ë©´ raw textë¡œë¶€í„° íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ NLPì—ì„œ í•„ìˆ˜ì 
    - unlabeled ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤ë©´ ë§ì€ ì‹œê°„ê³¼ ë¹„ìš©ì´ ì†Œìš”ë˜ëŠ” labelling ì‘ì—…ì„ ê±´ë„ˆ ë›¸ ìˆ˜ ìˆìŒ
    - ì™„ì „íˆ supervised learningì„ ëŒ€ì²´í•˜ì§€ ì•Šë”ë¼ë„ supervised learningì„ ìˆ˜í–‰í•  ë•Œ íš¨ìœ¨ì ì¸ featureë¡œ í™œìš©í•  ìˆ˜ ìˆìŒ
- unlabeled í…ìŠ¤íŠ¸ì—ì„œ í•™ìŠµì´ ì–´ë ¤ìš´ ì´ìœ 
    - ì–´ë–¤ ì†ì‹¤í•¨ìˆ˜ê°€ ë‹¤ì–‘í•œ taskì— íš¨ìœ¨ì ìœ¼ë¡œ transfer í•  ìˆ˜ ìˆëŠ” ëª©ì í•¨ìˆ˜ì¸ì§€ ë¶ˆë¶„ëª…
    - general tsakì— ëŒ€í•´ í•™ìŠµëœ í‘œí˜„ì„ ê°ê°ì˜ target taskì— ë§ê²Œ ë³€í˜•í•˜ëŠ” ê³¼ì •ì´ ë¶ˆë¶„ëª…
- GPT-1ì´ ì·¨í•œ ë°©ë²•ë¡ 
    - unsupervised pre-trainingê³¼ supervised fine-tuningì„ ì§„í–‰í•˜ëŠ” semi-supervised ë°©ë²•ë¡ 
    - GPT-1ì€ ëŒ€ëŸ‰ì˜ unlabeled í…ìŠ¤íŠ¸ ë°ì´í„°ì— ì ‘ê·¼ ê°€ëŠ¥í•˜ê³ , ìˆ˜ë™ìœ¼ë¡œ labellingëœ target taskì— ë§ëŠ” ë°ì´í„°ê°€ ìˆëŠ” ìƒí™©ì„ ê°€ì •
    - ë‹¤ì–‘í•œ Taskì— ëŒ€í•´ ì˜ ë™ì‘í•˜ê³  Long-Term contextë¥¼ ì˜ ê¸°ì–µí•˜ëŠ” ê²ƒì´ ê²€ì¦ëœ Transformer ëª¨ë¸ì„ ì‚¬ìš©
    - ëª¨ë¸ êµ¬ì¡° ë³€ê²½ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ì„œ ëª¨ë“  taskë¥¼ êµ¬ë¶„ ê¸°í˜¸(delimiter token)ë¥¼ í¬í•¨í•œ í•˜ë‚˜ì˜ ì—°ì†ëœ í…ìŠ¤íŠ¸ë¡œ ë³€ê²½í•´ì„œ í•™ìŠµ

### **Framework**

- íŠ¸ë ˆì´ë‹ ê³¼ì •ì˜ 2ê°€ì§€ stage
    1. ì²«ë²ˆì§¸ stageëŠ” ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ë¡œ ë†’ì€ ë²”ìš©ì„±ì˜ language modelì„ í•™ìŠµì‹œí‚´
    2. ë‘ë²ˆì§¸ stageëŠ” labeled ë°ì´í„°ë¥¼ ì´ìš©í•´ì„œ ê°ê°ì˜ taskì— ë§ê²Œ fine-tuningì„ ì§„í–‰
- Unsupervised pre-training
    - unlabeled í† í° corpus ğ’° = {ğ‘¢!, â€¦ , ğ‘¢"}, ì—ì„œ í‘œì¤€ ì ì¸ language modelling ëª©ì í•¨ìˆ˜ì¸ ì•„ë˜ ìˆ˜ì‹ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµ

      <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/c5c97d75-166d-40cf-8edc-09dc187932b1" width="300"/>

    - këŠ” context window í¬ê¸°, PëŠ” í•™ìŠµëœ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ì˜ íŒŒë¼ë¯¸í„° Î˜ì— ê¸°ë°˜í•œ conditional probability
    - í•™ìŠµì„ ìœ„í•´ Transformerì˜ ë³€í˜•ì¸ multi-layer Transformer decoderë¥¼ ì‚¬ìš©
- Supervised fine-tuning
    - ğ¿1ì†ì‹¤í•¨ìˆ˜ë¥¼ í†µí•´ì„œ unsupervised pre-trainingì„ ë§ˆì¹œ í›„ ê° target taskì— ë§ê²Œ supervised learningì„ ì§„í–‰
    - labeled ë°ì´í„°ì…‹ CëŠ” ì¸í’‹ í† í°ë“¤ì˜ ì‹œí€€ìŠ¤ ğ‘¥1, ğ‘¥2, . . , ğ‘¥mê³¼ label yë¡œ êµ¬ì„±
    - ì¸í’‹ì„ pre-trained ëª¨ë¸ì— ë„£ê³ , ë§ˆì§€ë§‰ transformer blockì˜ í™œì„±ê°’(activiton)ì¸ â„ ì„ ì–»ê³  ì´ëŠ” ë§ˆì§€ë§‰ linear layer íŒŒë¼ë¯¸í„°ì¸ ğ‘Šì™€ í•¨ê»˜ target yë¥¼ ì˜ˆì¸¡

      <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/85c1593f-5b26-414d-b8e0-68d5646b6293" width="300"/>

    - Supervised Learningì—ì„œ ìµœëŒ€í™”í•˜ë ¤ëŠ” ëª©ì í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ

      <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/5b5135c1-26ba-4dd3-99c1-493b213daf60" width="260"/>

    - ë˜í•œ fine-tuning ê³¼ì •ì—ì„œ ì•„ë˜ì™€ ê°™ì´ Language Modelling ëª©ì í•¨ìˆ˜ë¥¼ ì¶”ê°€ì ì¸ Auxiliary ëª©ì í•¨ìˆ˜ë¡œ ì§€ì •í•´ ì¤„ ê²½ìš° ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì„ ì–»ì„ ìˆ˜ ìˆìŒ

      <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/cbf21cdb-b17f-4e62-a64d-782af3dcd755" width="240"/>
 
        1. Supervised ëª¨ë¸ì˜ ì¼ë°˜í™”(generalization) ì„±ëŠ¥ì„ í–¥ìƒ
        2. ìˆ˜ë ´ ì†ë„ë¥¼ í–¥ìƒ
- í•™ìŠµì„ ìœ„í•œ Transformer ëª¨ë¸ êµ¬ì¡°

  <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/66a48be3-74b5-40a5-83b7-0d54b7ca26481" width="150"/>

    - Input: Text ì„ë² ë”© + Positional ì„ë² ë”©
    - Output: LMì˜ Prediction í™•ë¥ ê°’ + Sub Taskì— ëŒ€í•œ ì •ë‹µì„ ì˜ˆì¸¡

### **Task -specific input transformations**

- Inputì˜ ë³€í™˜ì´ í•„ìš”í•œ ì´ìœ 
    - ëª‡ëª‡ task, ì˜ˆë¥¼ ë“¤ì–´, text classification ê°™ì€ ê²½ìš°, ì•„ë¬´ëŸ° ìˆ˜ì •ì—†ì´ ì´ì „ì— ì–¸ê¸‰í•œ ë°©ì‹ìœ¼ë¡œ ë°”ë¡œ fine-tuningì„ ì§„í–‰
    - question answering, text entailmentì™€ ê°™ì€ taskëŠ” ì •ë ¬ëœ ë¬¸ì¥ ìŒ (sentence pairs)ë‚˜ (document, question, answer)ì˜ tripletsë¡œ êµ¬ì„±
    - ë”°ë¼ì„œ pre-trained ëª¨ë¸ì´ ì—°ì†ëœ í…ìŠ¤íŠ¸ì— ëŒ€í•´ í•™ìŠµë˜ì—ˆìœ¼ë¯€ë¡œ ì´ëŸ° taskì— ì ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì•½ê°„ì˜ ë³€í™˜ ê³¼ì •ì´ í•„ìš”
- GPT-1ì€ traversal-style approach ì·¨í•¨
    - ê°ê°ì˜ task íŠ¹ì„±ì— ë§ê²Œ êµ¬ì¡°í™”ëœ inputì„ pre-trained ëª¨ë¸ êµ¬ì¡°ê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì—°ì†ëœ í…ìŠ¤íŠ¸ë¡œ ë³€ê²½
    - ëª¨ë“  ë³€í™˜ì€ ì„ì˜ë¡œ ì´ˆê¸°í™”ëœ (randomly initialized)ê³¼ ì‹œì‘ í† í°(start token) ì¢…ë£Œ í† í°(end token) < ğ‘  >, < ğ‘’ > ì„ ì¶”ê°€
- ê°œë³„ Sub Taskì— ëŒ€í•´ ë³€í™˜ ê³¼ì • ì‚´í´ë³´ê¸°
    - Textual  entailment
        - premise ğ‘ì™€ hypothesis â„ë¥¼ delimiter token ($)ë¥¼ ì‚¬ì´ì— ë„£ê³  ì´ì–´ì¤Œ
    - Similarity
        - 2ê°€ì§€ ë¬¸ì¥(A,B) ì˜ ìˆœì„œê°€ ì¤‘ìš”í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, ëª¨ë¸ì´ Orderì— ëŒ€í•´ ì˜ëª»ëœ ì§€ì‹ì„ í•™ìŠµí•˜ì§€ ì•Šë„ë¡ 2ê°€ì§€ ìŒì„ ë§Œë“¦
        - A-B í•œ ìŒ, B-A í•œ ìŒ
    - Question Answering and Commonsense Reasoning
        - QA: document-question-answer
            - answerì˜ í›„ë³´êµ°ì´ ë§ìœ¼ë¯€ë¡œ í›„ë³´êµ° ë§ˆë‹¤ ìŒì„ ë§Œë“¦
        
        <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/72b7e535-9a3e-4e6d-aa51-e11c0a68ed4b" width="400"/>


### **Experiment(ì‹¤í—˜ í™˜ê²½)**

- Unsupervised pre-training
    - BookCorpus ë°ì´í„°ì…‹ì„ ì‚¬ìš©
    - long-term informationì„ ì˜ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ì—°ì†ì ì¸ ê¸´ í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
- Model specifications
    - ëª¨ë¸ì€ original Transformerì— ê¸°ë°˜
    - 12ê°œì˜ ë ˆì´ì–´ë¥¼ ê°€ì§„ masked self-attention headsë¥¼ í¬í•¨í•œ decoder-only transformerë¥¼ ì‚¬ìš©
    - Position-wise feed-forward ë„¤íŠ¸ì›Œí¬ëŠ” 3072 ì°¨ì›ì˜ Inner statesë¥¼ ì‚¬ìš©
    - ìµœëŒ€ learning rateê°€ 2.5e-4ì¸ Adam optimizerë¥¼ ì‚¬ìš©
    - learning rateëŠ” 0ì—ì„œ ì²« 2000 stepê¹Œì§€ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ê³ , 0ê¹Œì§€ cosine scheduleë¡œ annealing
    - 512ê°œì˜ í† í°ìœ¼ë¡œ êµ¬ì„±ëœ ì—°ì†ëœ sequenceë¥¼ ëœë¤í•˜ê²Œ 64ê°œì”© ë½‘ì•„ minibatchë¥¼ êµ¬ì„±í•˜ê³  100ë²ˆì˜ epochë™ì•ˆ í•™ìŠµ
    - BPE(Byte Pair Encoding)ì„ ì´ìš©í•˜ì—¬ 4ë§Œë²ˆì˜ mergeë¥¼ í†µí•´ vocabulary ìƒì„±
        - Residul Embeddingê³¼ attentionì˜ dropoutì€ 0.1
    - L2 ì •ê·œí™” í…€ ì´ìš©
    - í™œì„±í™” í•¨ìˆ˜ì— Gaussian Error Linear Unit(GELU) ì´ìš©
- Fine-tuning details
    - unsupervised pre-training ê³¼ì •ì—ì„œ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸
    í„°ë¥¼ ë‹¤ì‹œ ì‚¬ìš©
    - classifierì— dropout rateëŠ” 0.1ì„ ì ìš©
    - ëŒ€ë¶€ë¶„ì˜ Task ì— learning rate 6.25e-5ì™€ 32ì˜ batch sizeë¥¼ ì‚¬ìš©
    - ëŒ€ë¶€ë¶„ì˜ ê²½ìš° 3 epochsì˜ ë¹ ë¥¸ fine-tuningìœ¼ë¡œ ì¶©ë¶„

### Supervised fine-tuning ì„±ëŠ¥

- Natural Language Inference

    <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/86982a50-e44a-46e8-848f-693593cb6b55" width="330"/>
  
- Question answering and commonsense reasoning

    <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/11a3cf15-4a89-437d-a6ba-a7099e14a2e5" width="330"/>

- Semantic Similarity & Classification

    <img src="https://github.com/daunJJ/LLM_Study/assets/109944763/3d3834a5-2606-4c95-95b0-362738271909" width="330"/>
